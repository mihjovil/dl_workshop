{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously written code\n",
    "# Housing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_trn, y_trn), (x_tst, y_tst) = tf.keras.datasets.boston_housing.load_data()\n",
    "x_trn.shape, y_trn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Normalization\n",
    "$ \\tilde {X} = \\frac{X-\\mu_X}{\\sigma_X}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trn = (x_trn-x_trn.mean())/x_trn.std()\n",
    "x_tst = (x_tst-x_tst.mean())/x_tst.std()\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(20, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=opt, loss='mse', metrics=['mse'])\n",
    "history = model.fit(x=x_trn,\n",
    "                    y = y_trn,\n",
    "                    epochs=2000,\n",
    "                    verbose=0,\n",
    "                    validation_data=(x_tst,y_tst),\n",
    "                    batch_size=100\n",
    "                   )\n",
    "plt.plot(history.history['loss'], label='Training')\n",
    "plt.plot(history.history['val_loss'], label='Test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(x_tst)\n",
    "mse = ((y_hat-y_tst)**2).mean()\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_trn, y_trn)\n",
    "y_hat = lr.predict(x_tst)\n",
    "mse1 = ((y_hat-y_tst)**2).mean()\n",
    "print(\"The mse for the NN is {}, for a linear regression is {}\".format(mse, mse1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self made dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.datasets as ds\n",
    "# For regression dataset generator\n",
    "sample_number = 1000\n",
    "feature_number = 1000\n",
    "noise = 1e0\n",
    "# For classification generator\n",
    "class_separation = 1\n",
    "redundant_features = feature_number//3 # That would a third of the features are redundant\n",
    "repeated_features = feature_number // 10 \n",
    "number_classes = 10\n",
    "flip = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "x,y = ds.make_regression(n_samples=sample_number,\n",
    "                             n_features=feature_number,\n",
    "                             noise=noise,\n",
    "                        )\n",
    "x_trn, x_tst, y_trn, y_tst = train_test_split(x,y, test_size=0.2)\n",
    "print(x_trn.shape, y_trn.shape, x_tst.shape, y_tst.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model (hopefully beat, at least get close to it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(x_trn, y_trn)\n",
    "y_hat = model.predict(x_tst)\n",
    "mse = ((y_hat-y_tst)**2).mean()\n",
    "model.score(x_tst, y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification\n",
    "x,y = ds.make_classification(n_samples=sample_number,\n",
    "                                 n_features=feature_number,\n",
    "                                 n_repeated=repeated_features,\n",
    "                                 class_sep=class_separation,\n",
    "                                 n_redundant=redundant_features,\n",
    "                                 flip_y=flip\n",
    "                                 )\n",
    "x_trn, x_tst, y_trn, y_tst = train_test_split(x,y, test_size=0.2)\n",
    "print(x_trn.shape, y_trn.shape, x_tst.shape, y_tst.shape)\n",
    "# Baseline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "model = LogisticRegression()\n",
    "model.fit(x_trn, y_trn)\n",
    "y_hat = model.predict(x_tst)\n",
    "f1 = f1_score(y_hat, y_tst)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
